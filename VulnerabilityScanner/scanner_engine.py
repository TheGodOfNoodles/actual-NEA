"""Scanning engine for the NEA Web Vulnerability Scanner.

This module provides a lightweight crawler that performs a handful of
foundational security checks against a target website. The aim is to keep the
implementation easy to understand while still exercising real HTTP requests
and form submissions so the UI can surface meaningful findings.

Use this tool ethically and only against systems that you own or have explicit
permission to assess.
"""

from __future__ import annotations

import re
from collections import deque
from typing import Callable, Dict, Iterable, List, Optional, Set, Tuple
from urllib.parse import urljoin, urlparse, urlunparse

import requests
from bs4 import BeautifulSoup
from requests import Response

Vulnerability = Dict[str, str]
ProgressCallback = Callable[[float], None]
StatusCallback = Callable[[str], None]


def _safe_callback(callback: Callable, *args, **kwargs) -> None:
    """Invoke the callback and swallow any exception raised from the UI side."""
    if not callable(callback):
        return
    try:
        callback(*args, **kwargs)
    except Exception:
        # The UI should handle its own logging; silently ignore here to avoid
        # crashing the worker thread.
        pass


def _normalise_url(raw_url: str) -> str:
    """Return a consistent, fragment-free representation of a URL."""
    if not raw_url:
        return ""
    try:
        parsed = urlparse(raw_url)
    except ValueError:
        return ""

    if parsed.scheme not in {"http", "https"} or not parsed.netloc:
        return ""

    path = parsed.path or "/"
    normalised = urlunparse(
        (
            parsed.scheme.lower(),
            parsed.netloc.lower(),
            path,
            parsed.params,
            parsed.query,
            "",  # strip fragments
        )
    )
    return normalised


class BasicScanner:
    """Very small-scale web scanner used by the desktop UI."""

    USER_AGENT = "NEA-Basic-Scanner/1.0"
    MAX_PAGES = 15
    MAX_FORMS_PER_PAGE = 5
    REQUEST_TIMEOUT = 8
    HTML_MIME_TYPES = {"text/html", "application/xhtml+xml"}
    ALLOWED_INPUT_TYPES = {
        "text",
        "search",
        "email",
        "url",
        "tel",
        "password",
        "number",
        "hidden",
    }
    REQUIRED_HEADERS: Dict[str, str] = {
        "Content-Security-Policy": "Define a CSP header to restrict permitted sources.",
        "X-Content-Type-Options": "Return X-Content-Type-Options=nosniff to prevent MIME probing.",
        "X-Frame-Options": "Set X-Frame-Options=DENY or SAMEORIGIN to mitigate clickjacking.",
        "Strict-Transport-Security": "Serve the site over HTTPS and enable HSTS for at least 6 months.",
    }
    SQL_ERROR_PATTERNS: Tuple[str, ...] = (
        r"sql syntax",
        r"mysql_fetch",
        r"warning: mysql",
        r"unclosed quotation mark",
        r"odbc sql server driver",
        r"pg_.*error",
        r"sqlite error",
        r"ORA-\d{4}",
    )
    XSS_PAYLOAD = "<svg/onload=alert('nea')>"
    SQL_PAYLOAD = "' OR '1'='1"

    def __init__(
        self,
        target_url: str,
        depth: int,
        progress_callback: ProgressCallback,
        status_callback: StatusCallback,
    ) -> None:
        self.target_url = _normalise_url(target_url)
        if not self.target_url:
            raise ValueError("Target URL must include http:// or https:// and a hostname.")

        self.max_depth = max(0, depth)
        self.progress_callback = progress_callback
        self.status_callback = status_callback

        self.session = requests.Session()
        self.session.headers.update({"User-Agent": self.USER_AGENT})

        self.visited: Set[str] = set()
        self.findings: List[Vulnerability] = []
        self._finding_keys: Set[Tuple[str, str]] = set()
        self._security_headers_checked = False

        self._update_origin(self.target_url)

    # ------------------------------------------------------------------ Public
    def run(self) -> List[Vulnerability]:
        """Execute the scan and collect vulnerability findings."""
        self._status("Fetching start page...")
        self._progress(0.05)

        start_response = self._fetch(self.target_url)
        if start_response is None:
            self._progress(1.0)
            return self.findings

        first_page_url = self._response_url(start_response) or self.target_url
        self._update_origin(first_page_url)

        self._status("Scanning start page...")
        soup = self._process_page(first_page_url, start_response)
        self._progress(0.15)

        pages_scanned = 1
        queue: deque[Tuple[str, int]] = deque()
        if soup is not None and self.max_depth > 0:
            for link in self._extract_links(first_page_url, soup):
                queue.append((link, 1))

        if queue:
            self._status("Crawling linked pages...")

        while queue and pages_scanned < self.MAX_PAGES:
            url, depth = queue.popleft()
            if url in self.visited:
                continue

            response = self._fetch(url)
            if response is None:
                continue

            effective_url = self._response_url(response) or url
            soup = self._process_page(effective_url, response)
            pages_scanned += 1

            crawl_progress = 0.15 + 0.7 * (pages_scanned / self.MAX_PAGES)
            self._progress(min(crawl_progress, 0.9))

            if soup is not None and depth < self.max_depth:
                for link in self._extract_links(effective_url, soup):
                    if link not in self.visited:
                        queue.append((link, depth + 1))

        self._status("Finalising checks...")
        self._progress(0.95)

        self._status("Scan complete!")
        self._progress(1.0)

        return self.findings

    # --------------------------------------------------------------- Processing
    def _fetch(self, url: str) -> Optional[Response]:
        normalised = _normalise_url(url)
        if not normalised or normalised in self.visited:
            return None

        self.visited.add(normalised)

        try:
            response = self.session.get(
                normalised,
                timeout=self.REQUEST_TIMEOUT,
                allow_redirects=True,
            )
        except requests.RequestException as exc:
            self._add_finding(
                {
                    "type": "Connection Error",
                    "severity": "High",
                    "url": normalised,
                    "description": f"Request failed ({exc.__class__.__name__}): {str(exc)[:140]}",
                    "recommendation": "Verify the target is reachable and retry the scan.",
                }
            )
            return None

        final_url = _normalise_url(response.url)
        if final_url and final_url not in self.visited:
            self.visited.add(final_url)
        # Attach for later helpers.
        response.scanned_url = final_url or normalised  # type: ignore[attr-defined]
        return response

    def _process_page(self, page_url: str, response: Response) -> Optional[BeautifulSoup]:
        self._check_security_headers(page_url, response)

        if not self._is_html(response):
            return None

        soup = BeautifulSoup(response.text, "html.parser")
        self._test_forms(page_url, soup)
        return soup

    def _extract_links(self, base_url: str, soup: BeautifulSoup) -> Iterable[str]:
        for anchor in soup.find_all("a", href=True):
            href = anchor.get("href", "").strip()
            if not href or href.startswith(("javascript:", "mailto:", "tel:")):
                continue
            joined = urljoin(base_url, href)
            normalised = _normalise_url(joined)
            if not normalised or not self._is_same_host(normalised):
                continue
            yield normalised

    def _test_forms(self, page_url: str, soup: BeautifulSoup) -> None:
        forms = soup.find_all("form")[: self.MAX_FORMS_PER_PAGE]
        for index, form in enumerate(forms, start=1):
            method = (form.get("method") or "get").lower()
            action = form.get("action") or ""
            target = _normalise_url(urljoin(page_url, action)) or page_url
            if not self._is_same_host(target):
                continue

            field_names = self._collect_field_names(form)
            if not field_names:
                continue

            xss_data = self._build_payload(field_names, self.XSS_PAYLOAD)
            xss_response = self._submit_form(target, method, xss_data)
            if self._payload_reflected(xss_response, self.XSS_PAYLOAD):
                self._add_finding(
                    {
                        "type": "Potential Reflected XSS",
                        "severity": "Medium",
                        "url": target,
                        "description": (
                            "The submitted script payload was reflected in the response, "
                            "suggesting insufficient output encoding."
                        ),
                        "recommendation": (
                            "Escape user-supplied data before rendering and consider enabling a Content "
                            "Security Policy."
                        ),
                    }
                )

            sql_data = self._build_payload(field_names, self.SQL_PAYLOAD)
            sql_response = self._submit_form(target, method, sql_data)
            if self._sql_error_detected(sql_response):
                self._add_finding(
                    {
                        "type": "Potential SQL Injection",
                        "severity": "High",
                        "url": target,
                        "description": (
                            "Database-oriented error messages were returned after sending crafted input, "
                            "which may indicate injectable queries."
                        ),
                        "recommendation": (
                            "Adopt parameterised statements and validate inputs on both the client and server."
                        ),
                    }
                )

    # --------------------------------------------------------------- Utilities
    def _status(self, message: str) -> None:
        _safe_callback(self.status_callback, message)

    def _progress(self, value: float) -> None:
        _safe_callback(self.progress_callback, max(0.0, min(1.0, float(value))))

    def _update_origin(self, url: str) -> None:
        normalised = _normalise_url(url)
        if not normalised:
            return
        parsed = urlparse(normalised)
        host = parsed.netloc.lower()
        self.origin = f"{parsed.scheme}://{host}"
        self.hostname = host

    def _is_same_host(self, url: str) -> bool:
        parsed = urlparse(url)
        return parsed.netloc.lower() == getattr(self, "hostname", parsed.netloc.lower())

    def _response_url(self, response: Response) -> str:
        return getattr(response, "scanned_url", _normalise_url(response.url))  # type: ignore[attr-defined]

    def _is_html(self, response: Response) -> bool:
        content_type = response.headers.get("Content-Type", "")
        mime = content_type.split(";")[0].strip().lower()
        if mime in self.HTML_MIME_TYPES:
            return True
        if not mime:
            snippet = response.text[:200].lower()
            return "<html" in snippet or "<!doctype html" in snippet
        return False

    def _collect_field_names(self, form) -> List[str]:
        names: List[str] = []
        seen: Set[str] = set()

        for element in form.find_all(["input", "textarea"]):
            name = element.get("name")
            if not name or name in seen:
                continue

            if element.name == "textarea":
                candidate = name
            else:
                input_type = (element.get("type") or "text").lower()
                if input_type not in self.ALLOWED_INPUT_TYPES:
                    continue
                candidate = name

            seen.add(candidate)
            names.append(candidate)

        return names

    def _build_payload(self, field_names: List[str], payload: str) -> Dict[str, str]:
        if not field_names:
            return {}
        default_value = "nea-test"
        first_field = field_names[0]
        return {name: (payload if name == first_field else default_value) for name in field_names}

    def _submit_form(self, url: str, method: str, data: Dict[str, str]) -> Optional[Response]:
        try:
            if method == "post":
                response = self.session.post(url, data=data, timeout=self.REQUEST_TIMEOUT, allow_redirects=True)
            else:
                response = self.session.get(url, params=data, timeout=self.REQUEST_TIMEOUT, allow_redirects=True)
            response.scanned_url = _normalise_url(response.url) or url  # type: ignore[attr-defined]
            return response
        except requests.RequestException:
            return None

    def _payload_reflected(self, response: Optional[Response], payload: str) -> bool:
        if response is None or not self._is_html(response):
            return False
        return payload in response.text

    def _sql_error_detected(self, response: Optional[Response]) -> bool:
        if response is None:
            return False
        if response.status_code >= 500:
            return True
        snippet = response.text[:4000]
        for pattern in self.SQL_ERROR_PATTERNS:
            if re.search(pattern, snippet, re.IGNORECASE):
                return True
        return False

    def _check_security_headers(self, url: str, response: Response) -> None:
        if self._security_headers_checked:
            return

        self._security_headers_checked = True

        headers = response.headers
        final_url = self._response_url(response) or url
        parsed = urlparse(final_url)
        is_https = parsed.scheme == "https"

        if not is_https:
            self._add_finding(
                {
                    "type": "Insecure Transport",
                    "severity": "Medium",
                    "url": final_url,
                    "description": "The site responded over HTTP, meaning traffic is not encrypted in transit.",
                    "recommendation": "Serve the application exclusively over HTTPS with a trusted certificate.",
                }
            )

        for header, recommendation in self.REQUIRED_HEADERS.items():
            if header == "Strict-Transport-Security" and not is_https:
                continue
            if header not in headers:
                severity = "Medium" if header == "Strict-Transport-Security" else "Low"
                self._add_finding(
                    {
                        "type": "Missing Security Header",
                        "severity": severity,
                        "url": final_url,
                        "description": f"The {header} header is not present in responses.",
                        "recommendation": recommendation,
                    }
                )

    def _add_finding(self, finding: Vulnerability) -> None:
        key = (finding.get("type", ""), finding.get("url", ""))
        if key in self._finding_keys:
            return
        self._finding_keys.add(key)
        self.findings.append(finding)


def run_full_scan(
    target_url: str,
    depth: int,
    progress_callback: ProgressCallback,
    status_callback: StatusCallback,
) -> List[Vulnerability]:
    """Entry point used by the UI thread to launch a scan."""
    scanner = BasicScanner(
        target_url=target_url,
        depth=depth,
        progress_callback=progress_callback,
        status_callback=status_callback,
    )
    return scanner.run()
